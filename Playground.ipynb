{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import PIL\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import copy\n",
    "import time\n",
    "#\n",
    "import config.config_flags as Config\n",
    "import data_load.data_provider as dp\n",
    "import runner as runner\n",
    "import utils.task_helper as th\n",
    "import utils.helper as helper\n",
    "import datasetconf as DC\n",
    "import TaskClass as TaskClass\n",
    "import Task as Task\n",
    "import TestNets as TestNets\n",
    "import maml as MAML\n",
    "import tasml as TASML\n",
    "import testing_routines as TESTING_ROUTINES\n",
    "#\n",
    "_ = importlib.reload(Config)\n",
    "_ = importlib.reload(th)\n",
    "_ = importlib.reload(dp)\n",
    "_ = importlib.reload(runner)\n",
    "_ = importlib.reload(helper)\n",
    "_ = importlib.reload(DC)\n",
    "_ = importlib.reload(TaskClass)\n",
    "_ = importlib.reload(Task)\n",
    "_ = importlib.reload(TestNets)\n",
    "_ = importlib.reload(MAML)\n",
    "_ = importlib.reload(TASML)\n",
    "_ = importlib.reload(TESTING_ROUTINES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tak Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Filtering top-m alpha weights and training tasks\n",
      "Fetching train embeddings\n",
      "Path fetched: ../embeddings/tieredImageNet/center/train_embeddings.pkl\n",
      " 31%|███       | 31/100 [00:00<00:00, 309.62it/s]Generating training tasks\n",
      "Generating top m training tasks for test task 0\n",
      "100%|██████████| 100/100 [00:00<00:00, 521.46it/s]\n",
      " 86%|████████▌ | 86/100 [00:00<00:00, 856.81it/s]Generating top m training tasks for test task 1\n",
      "100%|██████████| 100/100 [00:00<00:00, 485.10it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 665.25it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 2\n",
      "Generating top m training tasks for test task 3\n",
      "100%|██████████| 100/100 [00:00<00:00, 791.48it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 715.18it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 4\n",
      "Generating top m training tasks for test task 5\n",
      "100%|██████████| 100/100 [00:00<00:00, 792.13it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 804.08it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 6\n",
      "Generating top m training tasks for test task 7\n",
      "100%|██████████| 100/100 [00:00<00:00, 860.62it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 862.88it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 8\n",
      "Generating top m training tasks for test task 9\n",
      "100%|██████████| 100/100 [00:00<00:00, 885.48it/s]\n",
      " 59%|█████▉    | 59/100 [00:00<00:00, 363.47it/s]Generating top m training tasks for test task 10\n",
      "100%|██████████| 100/100 [00:00<00:00, 440.91it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 738.08it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 11\n",
      "Generating top m training tasks for test task 12\n",
      "100%|██████████| 100/100 [00:00<00:00, 858.38it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 887.49it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 13\n",
      "Generating top m training tasks for test task 14\n",
      "100%|██████████| 100/100 [00:00<00:00, 597.28it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 782.66it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 15\n",
      "Generating top m training tasks for test task 16\n",
      "100%|██████████| 100/100 [00:00<00:00, 864.48it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 883.25it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 17\n",
      "Generating top m training tasks for test task 18\n",
      "100%|██████████| 100/100 [00:00<00:00, 880.92it/s]\n",
      " 27%|██▋       | 27/100 [00:00<00:00, 181.99it/s]Generating top m training tasks for test task 19\n",
      "100%|██████████| 100/100 [00:00<00:00, 411.29it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 868.21it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 20\n",
      "Generating top m training tasks for test task 21\n",
      "100%|██████████| 100/100 [00:00<00:00, 715.12it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 887.68it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 22\n",
      "Generating top m training tasks for test task 23\n",
      "100%|██████████| 100/100 [00:00<00:00, 856.37it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 808.22it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 24\n",
      "Generating top m training tasks for test task 25\n",
      "100%|██████████| 100/100 [00:00<00:00, 849.48it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 832.16it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 26\n",
      "Generating top m training tasks for test task 27\n",
      "100%|██████████| 100/100 [00:00<00:00, 403.52it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 805.01it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 28\n",
      "Generating top m training tasks for test task 29\n",
      "100%|██████████| 100/100 [00:00<00:00, 868.95it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 882.84it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 30\n",
      "Generating top m training tasks for test task 31\n",
      "100%|██████████| 100/100 [00:00<00:00, 849.07it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 865.43it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 32\n",
      "Generating top m training tasks for test task 33\n",
      "100%|██████████| 100/100 [00:00<00:00, 857.46it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 706.44it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 34\n",
      "Generating top m training tasks for test task 35\n",
      "100%|██████████| 100/100 [00:00<00:00, 853.19it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 826.62it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 36\n",
      "Generating top m training tasks for test task 37\n",
      "100%|██████████| 100/100 [00:00<00:00, 770.31it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 38\n",
      "100%|██████████| 100/100 [00:00<00:00, 307.20it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 830.55it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 39\n",
      "Generating top m training tasks for test task 40\n",
      "100%|██████████| 100/100 [00:00<00:00, 860.12it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 855.46it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 41\n",
      "Generating top m training tasks for test task 42\n",
      "100%|██████████| 100/100 [00:00<00:00, 719.06it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 812.10it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 43\n",
      "Generating top m training tasks for test task 44\n",
      "100%|██████████| 100/100 [00:00<00:00, 769.50it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 777.69it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 45\n",
      "Generating top m training tasks for test task 46\n",
      "100%|██████████| 100/100 [00:00<00:00, 848.25it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 868.86it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 47\n",
      "Generating top m training tasks for test task 48\n",
      "100%|██████████| 100/100 [00:00<00:00, 873.82it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 872.08it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 49\n",
      "Generating top m training tasks for test task 50\n",
      "100%|██████████| 100/100 [00:00<00:00, 314.08it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 842.20it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 51\n",
      "Generating top m training tasks for test task 52\n",
      "100%|██████████| 100/100 [00:00<00:00, 867.30it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 860.84it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 53\n",
      "Generating top m training tasks for test task 54\n",
      "100%|██████████| 100/100 [00:00<00:00, 806.44it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 707.67it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 55\n",
      "Generating top m training tasks for test task 56\n",
      "100%|██████████| 100/100 [00:00<00:00, 643.55it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 770.97it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 57\n",
      "Generating top m training tasks for test task 58\n",
      "100%|██████████| 100/100 [00:00<00:00, 737.26it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 721.54it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 59\n",
      "Generating top m training tasks for test task 60\n",
      "100%|██████████| 100/100 [00:00<00:00, 744.71it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 707.85it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 61\n",
      "Generating top m training tasks for test task 62\n",
      "100%|██████████| 100/100 [00:00<00:00, 799.16it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 712.73it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 63\n",
      "Generating top m training tasks for test task 64\n",
      "100%|██████████| 100/100 [00:00<00:00, 523.63it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 669.12it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 65\n",
      "Generating top m training tasks for test task 66\n",
      "100%|██████████| 100/100 [00:00<00:00, 238.38it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 669.10it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 67\n",
      "Generating top m training tasks for test task 68\n",
      "100%|██████████| 100/100 [00:00<00:00, 547.98it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 732.64it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 69\n",
      "Generating top m training tasks for test task 70\n",
      "100%|██████████| 100/100 [00:00<00:00, 749.85it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 833.54it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 71\n",
      "Generating top m training tasks for test task 72\n",
      "100%|██████████| 100/100 [00:00<00:00, 774.92it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 761.49it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 73\n",
      "Generating top m training tasks for test task 74\n",
      "100%|██████████| 100/100 [00:00<00:00, 539.99it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 524.63it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 75\n",
      "Generating top m training tasks for test task 76\n",
      "100%|██████████| 100/100 [00:00<00:00, 418.16it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 640.15it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 77\n",
      "Generating top m training tasks for test task 78\n",
      "100%|██████████| 100/100 [00:00<00:00, 730.56it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 798.49it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 79\n",
      "Generating top m training tasks for test task 80\n",
      "100%|██████████| 100/100 [00:00<00:00, 659.55it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 749.65it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 81\n",
      "Generating top m training tasks for test task 82\n",
      "100%|██████████| 100/100 [00:00<00:00, 738.64it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 814.31it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 83\n",
      "Generating top m training tasks for test task 84\n",
      "100%|██████████| 100/100 [00:00<00:00, 735.69it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 641.96it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 85\n",
      "Generating top m training tasks for test task 86\n",
      "100%|██████████| 100/100 [00:00<00:00, 213.48it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 632.10it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 87\n",
      "Generating top m training tasks for test task 88\n",
      "100%|██████████| 100/100 [00:00<00:00, 694.68it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 800.79it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 89\n",
      "Generating top m training tasks for test task 90\n",
      "100%|██████████| 100/100 [00:00<00:00, 757.27it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 816.77it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 91\n",
      "Generating top m training tasks for test task 92\n",
      "100%|██████████| 100/100 [00:00<00:00, 755.04it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 782.72it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 93\n",
      "Generating top m training tasks for test task 94\n",
      "100%|██████████| 100/100 [00:00<00:00, 728.35it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 793.61it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 95\n",
      "Generating top m training tasks for test task 96\n",
      "100%|██████████| 100/100 [00:00<00:00, 780.89it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 765.28it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 97\n",
      "Generating top m training tasks for test task 98\n",
      "100%|██████████| 100/100 [00:00<00:00, 807.47it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 800.27it/s]\n",
      "Generating top m training tasks for test task 99\n",
      "  2%|▏         | 2/100 [00:00<00:05, 16.45it/s]Fetching test embeddings\n",
      "Path fetched: ../embeddings/tieredImageNet/center/test_embeddings.pkl\n",
      "Generating test tasks\n",
      "100%|██████████| 100/100 [00:04<00:00, 24.69it/s]\n"
     ]
    }
   ],
   "source": [
    "#Set tensor device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Generate Task DB or load from filesystem\n",
    "#print(\"Generating tasks and compute their alpha weights\")\n",
    "#runner.populate_db()\n",
    "\n",
    "#Perform top-m filtering\n",
    "print(\"Filtering top-m alpha weights and training tasks\")\n",
    "alpha_weights, train_db = runner.top_m_filtering()\n",
    "\n",
    "#Get target task from filesystem\n",
    "test_path = helper.get_task_dataset_path(\"test\")\n",
    "test_db = runner.unpickle(test_path)\n",
    "\n",
    "#Get train data embeddings\n",
    "print(\"Fetching train embeddings\")\n",
    "train_provider = dp.DataProvider(\"train\", debug=False, verbose=False)\n",
    "train_tr_size = Config.TRAINING_NUM_OF_EXAMPLES_PER_CLASS\n",
    "train_val_size = Config.VALIDATION_NUM_OF_EXAMPLES_PER_CLASS\n",
    "print(\"Generating training tasks\")\n",
    "num_test_tasks = alpha_weights.shape[1]\n",
    "train_tasks = []\n",
    "for n in range(num_test_tasks): \n",
    "    print(\"Generating top m training tasks for test task \" + str(n))\n",
    "    train_tasks.append(th.generate_tasks(train_db[n], train_provider, train_tr_size, train_val_size, device))\n",
    "del train_db, train_provider #Free up space\n",
    "\n",
    "#Get train data embeddings\n",
    "print(\"Fetching test embeddings\")\n",
    "test_provider = dp.DataProvider(\"test\", debug=False, verbose=False)\n",
    "test_tr_size = Config.TRAINING_NUM_OF_EXAMPLES_PER_CLASS\n",
    "test_val_size = Config.TEST_VALIDATION_NUM_OF_EXAMPLES_PER_CLASS\n",
    "print(\"Generating test tasks\")\n",
    "test_tasks = th.generate_tasks(test_db, test_provider, test_tr_size, test_val_size, device) # Target tasks with only tests populated\n",
    "del test_db, test_provider #Free up space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network test instance.\n",
    "def get_test_net():\n",
    "    return TestNets.MAMLModule1(input_len=640, n_classes=Config.NUM_OF_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MAML, 0, 20.0, -, 19.23, 54.3, 9.646572828292847, 0.11289310455322266, 9.75946593284607\n",
      "TASML, 0, 20.0, 19.23, 24.53, 49.5, 8.76269793510437, 49.39261317253113, 0.10552597045898438, 58.26083707809448\n",
      "MAML, 1, 20.0, -, 17.97, 41.0, 9.475490093231201, 0.11696004867553711, 9.592450141906738\n",
      "TASML, 1, 20.0, 17.97, 27.53, 51.9, 9.13425087928772, 49.06350898742676, 0.10350799560546875, 58.301267862319946\n",
      "MAML, 2, 20.0, -, 20.83, 31.3, 8.386034965515137, 0.10630321502685547, 8.492338180541992\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4fd613eed479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mTESTING_ROUTINES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_maml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_net_maml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_tasks_for_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_target_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mTESTING_ROUTINES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_tasml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_net_tasml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_tasks_for_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_target_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_weights_for_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/MEP2G17/Modules/COMP6248/CW/COMP6248-CW-MetaLearning-SP/testing_routines.py\u001b[0m in \u001b[0;36mrun_tasml\u001b[0;34m(test_net, training_tasks, target_task, alpha_weights, test_task, isMetaFinetuned)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0macc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_of_training_module_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;31m# Warm-start.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     MAML.maml_nn_classifier_learn(\n\u001b[1;32m     76\u001b[0m         \u001b[0mtest_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MEP2G17/Modules/COMP6248/CW/COMP6248-CW-MetaLearning-SP/maml.py\u001b[0m in \u001b[0;36mmaml_nn_classifier_learn\u001b[0;34m(test_net, tasks, convergence_diff, max_meta_epochs, inner_epochs, inner_lr, outer_lr, loss_function)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m# Other loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mouter_loss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# Reload theta.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MEP2G17/Modules/COMP6248/CW/COMP6248-CW-MetaLearning-SP/TestNets.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2149\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2150\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Iterate each test task\n",
    "#Fetch the training tasks and weights for the test task\n",
    "for test_task_num, target_task in enumerate(test_tasks):\n",
    "    alpha_weights_for_target = alpha_weights[:,test_task_num]\n",
    "    training_tasks_for_target = train_tasks[test_task_num] #returns list\n",
    "    \n",
    "    # Remaps to utilisation of only training images for support and query.\n",
    "    training_target_task = Task.Task(task_friendly_name=target_task.task_friendly_name, batch_size=target_task.batch_size) \n",
    "    training_target_task.supp_train = target_task.supp_train\n",
    "    training_target_task.supp_targets = target_task.supp_targets\n",
    "    training_target_task.query_train = target_task.supp_train\n",
    "    training_target_task.query_targets = target_task.supp_targets\n",
    "    #\n",
    "    test_target_task = target_task\n",
    "\n",
    "    for training_task_num, training_task in enumerate(training_tasks_for_target):\n",
    "        alpha_weight = alpha_weights_for_target[training_task_num]\n",
    "        support = training_task.task_classes[0].support_imgs # Get support set of class 0\n",
    "        query = training_task.task_classes[0].query_imgs\n",
    "        \n",
    "        # if training_task_num < 5 and test_task_num==0: #Print weight and shape to get a feel of the data structures\n",
    "        #     print(\"alpha weight: \", alpha_weight, \"Support length: \", len(support), \"Query length: \", len(query))\n",
    "\n",
    "    # Test nn modules (equal).\n",
    "    test_net_maml = get_test_net() \n",
    "    #\n",
    "    test_net_tasml = get_test_net()\n",
    "    test_net_tasml.load_state_dict(copy.deepcopy(test_net_maml.state_dict()))\n",
    "\n",
    "    TESTING_ROUTINES.run_maml(test_net_maml, training_tasks_for_target, training_target_task, test_target_task)\n",
    "    TESTING_ROUTINES.run_tasml(test_net_tasml, training_tasks_for_target, training_target_task, alpha_weights_for_target, test_target_task)"
   ]
  },
  {
   "source": [
    "## Loading tasks from local img folders.\n",
    "```python\n",
    "task1 = Task.create_task_given(\n",
    "    task_friendly_name='Task1',\n",
    "    dataset_name='boat',\n",
    "    class_names=['Gondola', 'Motopontonerettangolare'], #n-way\n",
    "    len_support_dataset=3, #k-shot\n",
    "    len_query_dataset=2,   #k-shot\n",
    "    let_test_dataset=5, # Number of test cases per class\n",
    "    transformer=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(224),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225])]),\n",
    "    img_size=224,\n",
    "    start_class_id=0)\n",
    "task1.reset_train_session()\n",
    "#\n",
    "task2 = Task.create_task_given(\n",
    "    task_friendly_name='Task2',\n",
    "    dataset_name='boat',\n",
    "    class_names=['Raccoltarifiuti', 'Water'],\n",
    "    len_support_dataset=3,\n",
    "    len_query_dataset=2,\n",
    "    let_test_dataset=5, # Number of test cases per class\n",
    "    transformer=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(224),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225])]),\n",
    "    img_size=224,\n",
    "    start_class_id=2)\n",
    "task2.reset_train_session()\n",
    "#\n",
    "TASKS = [task1, task2]\n",
    "LEN_CLASSES = sum(len(task.task_classes) for task in TASKS)\n",
    "CLASSES_NAMES = [taskclass.class_friendly_name for task in TASKS for taskclass in task.task_classes]\n",
    "NUM_IN_CHANNELS = 3 # RGB\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### MAML Runner:\n",
    "```python\n",
    "MAML.maml_nn_classifier_learn(\n",
    "    test_net: torch.nn.Module,\n",
    "    tasks: list[Task.Task],\n",
    "    convergence_diff: float = 0.0001,\n",
    "    max_meta_epochs = 10,\n",
    "    inner_epochs: int = 1,\n",
    "    inner_lr: float = 0.001,\n",
    "    outer_lr: float = 0.001,\n",
    "    loss_function = torch.nn.CrossEntropyLoss()):\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### TASML Runner:\n",
    "```python\n",
    "TASML.tasml_nn_classifier_learn(\n",
    "    test_net: torch.nn.Module,\n",
    "    tasks: list[Task.Task],\n",
    "    target_task: Task.Task,\n",
    "    alpha_weights: torch.Tensor,\n",
    "    convergence_diff: float = 0.0001,\n",
    "    max_meta_epochs = 10,\n",
    "    inner_epochs: int = 1,\n",
    "    inner_lr: float = 0.001,\n",
    "    outer_lr: float = 0.001,\n",
    "    loss_function = torch.nn.CrossEntropyLoss()):\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python394jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.9.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}