{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import PIL\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import copy\n",
    "import time\n",
    "#\n",
    "import config.config_flags as Config\n",
    "import data_load.data_provider as dp\n",
    "import runner as runner\n",
    "import utils.task_helper as th\n",
    "import utils.helper as helper\n",
    "import datasetconf as DC\n",
    "import TaskClass as TaskClass\n",
    "import Task as Task\n",
    "import TestNets as TestNets\n",
    "import maml as MAML\n",
    "import tasml as TASML\n",
    "import testing_routines as TESTING_ROUTINES\n",
    "#\n",
    "_ = importlib.reload(Config)\n",
    "_ = importlib.reload(th)\n",
    "_ = importlib.reload(dp)\n",
    "_ = importlib.reload(runner)\n",
    "_ = importlib.reload(helper)\n",
    "_ = importlib.reload(DC)\n",
    "_ = importlib.reload(TaskClass)\n",
    "_ = importlib.reload(Task)\n",
    "_ = importlib.reload(TestNets)\n",
    "_ = importlib.reload(MAML)\n",
    "_ = importlib.reload(TASML)\n",
    "_ = importlib.reload(TESTING_ROUTINES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tak Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fetch all training tasks\n",
      "Fetching train embeddings\n",
      "Path fetched: ../embeddings\\tieredImageNet\\center\\train_embeddings.pkl\n",
      "  1%|          | 2/300 [00:00<00:15, 19.74it/s]Generating training tasks\n",
      "Generating all training tasks\n",
      "100%|██████████| 300/300 [00:03<00:00, 92.38it/s] \n",
      "Fetching test embeddings\n",
      "Path fetched: ../embeddings\\tieredImageNet\\center\\test_embeddings.pkl\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating test tasks\n",
      "100%|██████████| 100/100 [00:35<00:00,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "#Flag to enable top M filtering\n",
    "top_m_filtering = False\n",
    "\n",
    "#Set tensor device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Generate Task DB or load from filesystem\n",
    "#print(\"Generating tasks and compute their alpha weights\")\n",
    "#runner.populate_db()\n",
    "\n",
    "#Perform top-m filtering\n",
    "if top_m_filtering == True:\n",
    "    print(\"Filtering top-m alpha weights and training tasks\")\n",
    "    alpha_weights, train_db = runner.top_m_filtering()\n",
    "else:\n",
    "    print(\"Fetch all training tasks\")\n",
    "    train_db = runner.get_task_dataset(\"train\")\n",
    "\n",
    "#Get target task from filesystem\n",
    "test_db = runner.get_task_dataset(\"test\")\n",
    "\n",
    "#Get train data embeddings\n",
    "print(\"Fetching train embeddings\")\n",
    "train_provider = dp.DataProvider(\"train\", debug=False, verbose=False)\n",
    "train_tr_size = Config.TRAINING_NUM_OF_EXAMPLES_PER_CLASS\n",
    "train_val_size = Config.VALIDATION_NUM_OF_EXAMPLES_PER_CLASS\n",
    "print(\"Generating training tasks\")\n",
    "train_tasks = []\n",
    "if top_m_filtering == True:\n",
    "    num_test_tasks = alpha_weights.shape[1]\n",
    "    for n in range(num_test_tasks): \n",
    "        print(\"Generating top m training tasks for test task \" + str(n))\n",
    "        train_tasks.append(th.generate_tasks(train_db[n], train_provider, train_tr_size, train_val_size, device))\n",
    "else:\n",
    "    print(\"Generating all training tasks\")\n",
    "    train_tasks.append(th.generate_tasks(train_db, train_provider, train_tr_size, train_val_size, device))\n",
    "\n",
    "del train_db, train_provider #Free up space\n",
    "\n",
    "#Get train data embeddings\n",
    "print(\"Fetching test embeddings\")\n",
    "test_provider = dp.DataProvider(\"test\", debug=False, verbose=False)\n",
    "test_tr_size = Config.TRAINING_NUM_OF_EXAMPLES_PER_CLASS\n",
    "test_val_size = Config.TEST_VALIDATION_NUM_OF_EXAMPLES_PER_CLASS\n",
    "print(\"Generating test tasks\")\n",
    "test_tasks = th.generate_tasks(test_db, test_provider, test_tr_size, test_val_size, device) # Target tasks with only tests populated\n",
    "del test_db, test_provider #Free up space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network test instance.\n",
    "def get_test_net():\n",
    "    return TestNets.MAMLModule1(input_len=640, n_classes=Config.NUM_OF_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Support length:  1 Query length:  15\nSupport length:  1 Query length:  15\nSupport length:  1 Query length:  15\nSupport length:  1 Query length:  15\nSupport length:  1 Query length:  15\n"
     ]
    }
   ],
   "source": [
    "#Iterate each test task\n",
    "#Fetch the training tasks and weights for the test task\n",
    "for test_task_num, target_task in enumerate(test_tasks):\n",
    "    if top_m_filtering == True:\n",
    "        alpha_weights_for_target = alpha_weights[:,test_task_num]\n",
    "        training_tasks_for_target = train_tasks[test_task_num] # Returns filtered list of training tasks for the specific test task\n",
    "    else:\n",
    "        training_tasks_for_target = train_tasks[0] # Returns the same (and only) full list of training tasks\n",
    "    \n",
    "    # Remaps to utilisation of only training images for support and query.\n",
    "    training_target_task = Task.Task(task_friendly_name=target_task.task_friendly_name, batch_size=target_task.batch_size) \n",
    "    training_target_task.supp_train = target_task.supp_train\n",
    "    training_target_task.supp_targets = target_task.supp_targets\n",
    "    training_target_task.query_train = target_task.supp_train\n",
    "    training_target_task.query_targets = target_task.supp_targets\n",
    "    #\n",
    "    test_target_task = target_task\n",
    "\n",
    "    for training_task_num, training_task in enumerate(training_tasks_for_target):\n",
    "        if top_m_filtering == True:\n",
    "            alpha_weight = alpha_weights_for_target[training_task_num]\n",
    "        support = training_task.task_classes[0].support_imgs # Get support set of class 0\n",
    "        query = training_task.task_classes[0].query_imgs\n",
    "        \n",
    "        if training_task_num < 5 and test_task_num==0: #Print weight and shape to get a feel of the data structures\n",
    "            if top_m_filtering == True:\n",
    "                print(\"alpha weight: \", alpha_weight, \"Support length: \", len(support), \"Query length: \", len(query))\n",
    "            else:\n",
    "                print(\"Support length: \", len(support), \"Query length: \", len(query))\n",
    "\n",
    "    # Test nn modules (equal).\n",
    "    test_net_base = get_test_net()\n",
    "    #\n",
    "    test_net_maml = get_test_net()\n",
    "    test_net_maml.load_state_dict(copy.deepcopy(test_net_base.state_dict()))\n",
    "    #\n",
    "    test_net_maml_ft = get_test_net()\n",
    "    test_net_maml_ft.load_state_dict(copy.deepcopy(test_net_base.state_dict()))\n",
    "    #\n",
    "    test_net_tasml = get_test_net()\n",
    "    test_net_tasml.load_state_dict(copy.deepcopy(test_net_maml.state_dict()))\n",
    "    #\n",
    "    test_net_tasml_ft = get_test_net()\n",
    "    test_net_tasml_ft.load_state_dict(copy.deepcopy(test_net_base.state_dict()))\n",
    "\n",
    "    TESTING_ROUTINES.run_baselearner(test_net_base, training_tasks_for_target, training_target_task, test_target_task)\n",
    "    TESTING_ROUTINES.run_maml(test_net_maml, training_tasks_for_target, training_target_task, test_target_task, isMetaFinetuned=True)\n",
    "    TESTING_ROUTINES.run_maml(test_net_maml_ft, training_tasks_for_target, training_target_task, test_target_task, isMetaFinetuned=False)\n",
    "    TESTING_ROUTINES.run_tasml(test_net_tasml, training_tasks_for_target, training_target_task, alpha_weights_for_target, test_target_task, isMetaFinetuned=True)\n",
    "    TESTING_ROUTINES.run_tasml(test_net_tasml_ft, training_tasks_for_target, training_target_task, alpha_weights_for_target, test_target_task, isMetaFinetuned=False)"
   ]
  },
  {
   "source": [
    "## Loading tasks from local img folders.\n",
    "```python\n",
    "task1 = Task.create_task_given(\n",
    "    task_friendly_name='Task1',\n",
    "    dataset_name='boat',\n",
    "    class_names=['Gondola', 'Motopontonerettangolare'], #n-way\n",
    "    len_support_dataset=3, #k-shot\n",
    "    len_query_dataset=2,   #k-shot\n",
    "    let_test_dataset=5, # Number of test cases per class\n",
    "    transformer=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(224),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225])]),\n",
    "    img_size=224,\n",
    "    start_class_id=0)\n",
    "task1.reset_train_session()\n",
    "#\n",
    "task2 = Task.create_task_given(\n",
    "    task_friendly_name='Task2',\n",
    "    dataset_name='boat',\n",
    "    class_names=['Raccoltarifiuti', 'Water'],\n",
    "    len_support_dataset=3,\n",
    "    len_query_dataset=2,\n",
    "    let_test_dataset=5, # Number of test cases per class\n",
    "    transformer=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(224),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225])]),\n",
    "    img_size=224,\n",
    "    start_class_id=2)\n",
    "task2.reset_train_session()\n",
    "#\n",
    "TASKS = [task1, task2]\n",
    "LEN_CLASSES = sum(len(task.task_classes) for task in TASKS)\n",
    "CLASSES_NAMES = [taskclass.class_friendly_name for task in TASKS for taskclass in task.task_classes]\n",
    "NUM_IN_CHANNELS = 3 # RGB\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### MAML Runner:\n",
    "```python\n",
    "MAML.maml_nn_classifier_learn(\n",
    "    test_net: torch.nn.Module,\n",
    "    tasks: list[Task.Task],\n",
    "    convergence_diff: float = 0.0001,\n",
    "    max_meta_epochs = 10,\n",
    "    inner_epochs: int = 1,\n",
    "    inner_lr: float = 0.001,\n",
    "    outer_lr: float = 0.001,\n",
    "    loss_function = torch.nn.CrossEntropyLoss()):\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### TASML Runner:\n",
    "```python\n",
    "TASML.tasml_nn_classifier_learn(\n",
    "    test_net: torch.nn.Module,\n",
    "    tasks: list[Task.Task],\n",
    "    target_task: Task.Task,\n",
    "    alpha_weights: torch.Tensor,\n",
    "    convergence_diff: float = 0.0001,\n",
    "    max_meta_epochs = 10,\n",
    "    inner_epochs: int = 1,\n",
    "    inner_lr: float = 0.001,\n",
    "    outer_lr: float = 0.001,\n",
    "    loss_function = torch.nn.CrossEntropyLoss()):\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd0948d73eb6dfda2e804d788ca8e9f193cc8a7762f6fd3fe59d9c3fe882d54f9f4",
   "display_name": "Python 3.8.8 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}