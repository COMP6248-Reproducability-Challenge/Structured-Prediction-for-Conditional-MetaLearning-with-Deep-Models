{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import PIL\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import copy\n",
    "#\n",
    "import config.config_flags as Config\n",
    "import data_load.data_provider as dp\n",
    "import runner as runner\n",
    "import utils.task_helper as th\n",
    "import utils.helper as helper\n",
    "import datasetconf as DC\n",
    "import TaskClass as TaskClass\n",
    "import Task as Task\n",
    "import TestNets as TestNets\n",
    "import maml as MAML\n",
    "import tasml as TASML\n",
    "#\n",
    "_ = importlib.reload(Config)\n",
    "_ = importlib.reload(th)\n",
    "_ = importlib.reload(dp)\n",
    "_ = importlib.reload(runner)\n",
    "_ = importlib.reload(helper)\n",
    "_ = importlib.reload(DC)\n",
    "_ = importlib.reload(TaskClass)\n",
    "_ = importlib.reload(Task)\n",
    "_ = importlib.reload(TestNets)\n",
    "_ = importlib.reload(MAML)\n",
    "_ = importlib.reload(TASML)"
   ]
  },
  {
   "source": [
    "## Loading tasks from local img folders.\n",
    "```python\n",
    "task1 = Task.create_task_given(\n",
    "    task_friendly_name='Task1',\n",
    "    dataset_name='boat',\n",
    "    class_names=['Gondola', 'Motopontonerettangolare'], #n-way\n",
    "    len_support_dataset=3, #k-shot\n",
    "    len_query_dataset=2,   #k-shot\n",
    "    let_test_dataset=5, # Number of test cases per class\n",
    "    transformer=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(224),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225])]),\n",
    "    img_size=224,\n",
    "    start_class_id=0)\n",
    "task1.reset_train_session()\n",
    "#\n",
    "task2 = Task.create_task_given(\n",
    "    task_friendly_name='Task2',\n",
    "    dataset_name='boat',\n",
    "    class_names=['Raccoltarifiuti', 'Water'],\n",
    "    len_support_dataset=3,\n",
    "    len_query_dataset=2,\n",
    "    let_test_dataset=5, # Number of test cases per class\n",
    "    transformer=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(224),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225])]),\n",
    "    img_size=224,\n",
    "    start_class_id=2)\n",
    "task2.reset_train_session()\n",
    "#\n",
    "TASKS = [task1, task2]\n",
    "LEN_CLASSES = sum(len(task.task_classes) for task in TASKS)\n",
    "CLASSES_NAMES = [taskclass.class_friendly_name for task in TASKS for taskclass in task.task_classes]\n",
    "NUM_IN_CHANNELS = 3 # RGB\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tak Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Filtering top-m alpha weights and training tasks\n",
      "Fetching train embeddings\n",
      "Path fetched: ../embeddings/tieredImageNet/center/train_embeddings.pkl\n",
      " 32%|███▏      | 32/100 [00:00<00:00, 317.63it/s]Generating training tasks\n",
      "Generating top m training tasks for test task 0\n",
      "100%|██████████| 100/100 [00:00<00:00, 523.32it/s]\n",
      " 74%|███████▍  | 74/100 [00:00<00:00, 734.05it/s]Generating top m training tasks for test task 1\n",
      "100%|██████████| 100/100 [00:00<00:00, 440.27it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 718.17it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 2\n",
      "Generating top m training tasks for test task 3\n",
      "100%|██████████| 100/100 [00:00<00:00, 725.89it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 758.43it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 4\n",
      "Generating top m training tasks for test task 5\n",
      "100%|██████████| 100/100 [00:00<00:00, 727.65it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 766.47it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 6\n",
      "Generating top m training tasks for test task 7\n",
      "100%|██████████| 100/100 [00:00<00:00, 724.92it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 769.60it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 8\n",
      "Generating top m training tasks for test task 9\n",
      "100%|██████████| 100/100 [00:00<00:00, 753.04it/s]\n",
      " 60%|██████    | 60/100 [00:00<00:00, 337.04it/s]Generating top m training tasks for test task 10\n",
      "100%|██████████| 100/100 [00:00<00:00, 406.56it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 669.07it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 11\n",
      "Generating top m training tasks for test task 12\n",
      "100%|██████████| 100/100 [00:00<00:00, 763.97it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 793.13it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 13\n",
      "Generating top m training tasks for test task 14\n",
      "100%|██████████| 100/100 [00:00<00:00, 775.08it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 815.46it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 15\n",
      "Generating top m training tasks for test task 16\n",
      "100%|██████████| 100/100 [00:00<00:00, 783.61it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 820.66it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 17\n",
      "Generating top m training tasks for test task 18\n",
      "100%|██████████| 100/100 [00:00<00:00, 789.85it/s]\n",
      " 28%|██▊       | 28/100 [00:00<00:00, 173.70it/s]Generating top m training tasks for test task 19\n",
      "100%|██████████| 100/100 [00:00<00:00, 371.19it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 792.38it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 20\n",
      "Generating top m training tasks for test task 21\n",
      "100%|██████████| 100/100 [00:00<00:00, 790.07it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 792.67it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 22\n",
      "Generating top m training tasks for test task 23\n",
      "100%|██████████| 100/100 [00:00<00:00, 756.44it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 801.66it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 24\n",
      "Generating top m training tasks for test task 25\n",
      "100%|██████████| 100/100 [00:00<00:00, 778.53it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 782.34it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 26\n",
      "Generating top m training tasks for test task 27\n",
      "100%|██████████| 100/100 [00:00<00:00, 352.94it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 713.82it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 28\n",
      "Generating top m training tasks for test task 29\n",
      "100%|██████████| 100/100 [00:00<00:00, 761.02it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 767.38it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 30\n",
      "Generating top m training tasks for test task 31\n",
      "100%|██████████| 100/100 [00:00<00:00, 552.67it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 673.41it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 32\n",
      "Generating top m training tasks for test task 33\n",
      "100%|██████████| 100/100 [00:00<00:00, 722.32it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 772.20it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 34\n",
      "Generating top m training tasks for test task 35\n",
      "100%|██████████| 100/100 [00:00<00:00, 770.84it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 692.54it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 36\n",
      "Generating top m training tasks for test task 37\n",
      "100%|██████████| 100/100 [00:00<00:00, 729.97it/s]\n",
      " 20%|██        | 20/100 [00:00<00:00, 114.77it/s]Generating top m training tasks for test task 38\n",
      "100%|██████████| 100/100 [00:00<00:00, 356.24it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 844.09it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 39\n",
      "Generating top m training tasks for test task 40\n",
      "100%|██████████| 100/100 [00:00<00:00, 815.73it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 793.79it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 41\n",
      "Generating top m training tasks for test task 42\n",
      "100%|██████████| 100/100 [00:00<00:00, 778.48it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 836.26it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 43\n",
      "Generating top m training tasks for test task 44\n",
      "100%|██████████| 100/100 [00:00<00:00, 773.29it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 731.81it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 45\n",
      "Generating top m training tasks for test task 46\n",
      "100%|██████████| 100/100 [00:00<00:00, 809.17it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 807.15it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 47\n",
      "Generating top m training tasks for test task 48\n",
      "100%|██████████| 100/100 [00:00<00:00, 739.90it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 760.75it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 49\n",
      "Generating top m training tasks for test task 50\n",
      "100%|██████████| 100/100 [00:00<00:00, 294.66it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 689.92it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 51\n",
      "Generating top m training tasks for test task 52\n",
      "100%|██████████| 100/100 [00:00<00:00, 609.80it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 513.07it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 53\n",
      "Generating top m training tasks for test task 54\n",
      "100%|██████████| 100/100 [00:00<00:00, 475.17it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 750.62it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 55\n",
      "Generating top m training tasks for test task 56\n",
      "100%|██████████| 100/100 [00:00<00:00, 657.43it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 656.31it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 57\n",
      "Generating top m training tasks for test task 58\n",
      "100%|██████████| 100/100 [00:00<00:00, 752.65it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 610.85it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 59\n",
      "Generating top m training tasks for test task 60\n",
      "100%|██████████| 100/100 [00:00<00:00, 640.18it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 689.95it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 61\n",
      "Generating top m training tasks for test task 62\n",
      "100%|██████████| 100/100 [00:00<00:00, 771.26it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 786.88it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 63\n",
      "Generating top m training tasks for test task 64\n",
      "100%|██████████| 100/100 [00:00<00:00, 675.20it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 732.91it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 65\n",
      "Generating top m training tasks for test task 66\n",
      "100%|██████████| 100/100 [00:00<00:00, 231.71it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 637.91it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 67\n",
      "Generating top m training tasks for test task 68\n",
      "100%|██████████| 100/100 [00:00<00:00, 710.10it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 720.23it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 69\n",
      "Generating top m training tasks for test task 70\n",
      "100%|██████████| 100/100 [00:00<00:00, 622.88it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 559.56it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 71\n",
      "Generating top m training tasks for test task 72\n",
      "100%|██████████| 100/100 [00:00<00:00, 712.01it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 697.09it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 73\n",
      "Generating top m training tasks for test task 74\n",
      "100%|██████████| 100/100 [00:00<00:00, 667.72it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 559.67it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 75\n",
      "Generating top m training tasks for test task 76\n",
      "100%|██████████| 100/100 [00:00<00:00, 722.44it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 700.29it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 77\n",
      "Generating top m training tasks for test task 78\n",
      "100%|██████████| 100/100 [00:00<00:00, 618.47it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 785.57it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 79\n",
      "Generating top m training tasks for test task 80\n",
      "100%|██████████| 100/100 [00:00<00:00, 769.14it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 765.00it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 81\n",
      "Generating top m training tasks for test task 82\n",
      "100%|██████████| 100/100 [00:00<00:00, 744.27it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 814.79it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 83\n",
      "Generating top m training tasks for test task 84\n",
      "100%|██████████| 100/100 [00:00<00:00, 690.16it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 792.10it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 85\n",
      "Generating top m training tasks for test task 86\n",
      "100%|██████████| 100/100 [00:00<00:00, 233.66it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 802.77it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 87\n",
      "Generating top m training tasks for test task 88\n",
      "100%|██████████| 100/100 [00:00<00:00, 760.58it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 806.30it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 89\n",
      "Generating top m training tasks for test task 90\n",
      "100%|██████████| 100/100 [00:00<00:00, 755.44it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 774.84it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 91\n",
      "Generating top m training tasks for test task 92\n",
      "100%|██████████| 100/100 [00:00<00:00, 786.47it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 806.03it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 93\n",
      "Generating top m training tasks for test task 94\n",
      "100%|██████████| 100/100 [00:00<00:00, 784.74it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 807.54it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 95\n",
      "Generating top m training tasks for test task 96\n",
      "100%|██████████| 100/100 [00:00<00:00, 745.88it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 809.50it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Generating top m training tasks for test task 97\n",
      "Generating top m training tasks for test task 98\n",
      "100%|██████████| 100/100 [00:00<00:00, 769.03it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 791.55it/s]\n",
      "Generating top m training tasks for test task 99\n",
      "Fetching test embeddings\n",
      "Path fetched: ../embeddings/tieredImageNet/center/test_embeddings.pkl\n",
      "  2%|▏         | 2/100 [00:00<00:05, 17.36it/s]Generating test tasks\n",
      "100%|██████████| 100/100 [00:03<00:00, 25.04it/s]\n"
     ]
    }
   ],
   "source": [
    "#Set tensor device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Generate Task DB or load from filesystem\n",
    "#print(\"Generating tasks and compute their alpha weights\")\n",
    "#runner.populate_db()\n",
    "\n",
    "#Perform top-m filtering\n",
    "print(\"Filtering top-m alpha weights and training tasks\")\n",
    "alpha_weights, train_db = runner.top_m_filtering()\n",
    "\n",
    "#Get target task from filesystem\n",
    "test_path = helper.get_task_dataset_path(\"test\")\n",
    "test_db = runner.unpickle(test_path)\n",
    "\n",
    "#Get train data embeddings\n",
    "print(\"Fetching train embeddings\")\n",
    "train_provider = dp.DataProvider(\"train\", debug=False, verbose=False)\n",
    "train_tr_size = Config.TRAINING_NUM_OF_EXAMPLES_PER_CLASS\n",
    "train_val_size = Config.VALIDATION_NUM_OF_EXAMPLES_PER_CLASS\n",
    "print(\"Generating training tasks\")\n",
    "num_test_tasks = alpha_weights.shape[1]\n",
    "train_tasks = []\n",
    "for n in range(num_test_tasks): \n",
    "    print(\"Generating top m training tasks for test task \" + str(n))\n",
    "    train_tasks.append(th.generate_tasks(train_db[n], train_provider, train_tr_size, train_val_size, device))\n",
    "del train_db, train_provider #Free up space\n",
    "\n",
    "#Get train data embeddings\n",
    "print(\"Fetching test embeddings\")\n",
    "test_provider = dp.DataProvider(\"test\", debug=False, verbose=False)\n",
    "test_tr_size = Config.TRAINING_NUM_OF_EXAMPLES_PER_CLASS\n",
    "test_val_size = Config.TEST_VALIDATION_NUM_OF_EXAMPLES_PER_CLASS\n",
    "print(\"Generating test tasks\")\n",
    "test_tasks = th.generate_tasks(test_db, test_provider, test_tr_size, test_val_size, device) # Target tasks with only tests populated\n",
    "del test_db, test_provider #Free up space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network test instance.\n",
    "def get_test_net():\n",
    "    return TestNets.MAMLModule1(input_len=640, n_classes=Config.NUM_OF_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "alpha weight:  0.27781264684790585 Support length:  1 Query length:  15\n",
      "alpha weight:  0.23649674248923702 Support length:  1 Query length:  15\n",
      "alpha weight:  0.20492572563580325 Support length:  1 Query length:  15\n",
      "alpha weight:  0.20243367990656225 Support length:  1 Query length:  15\n",
      "alpha weight:  0.18424414353421525 Support length:  1 Query length:  15\n",
      "0.20000000298023224\n",
      "0.26233333349227905\n",
      "\n",
      "0.20000000298023224\n",
      "0.24633333086967468\n",
      "\n",
      "0.20000000298023224\n",
      "0.29366666078567505\n",
      "\n",
      "0.20000000298023224\n",
      "0.2759999930858612\n",
      "\n",
      "0.20000000298023224\n",
      "0.39666667580604553\n",
      "\n",
      "0.20000000298023224\n",
      "0.40933331847190857\n",
      "\n",
      "0.20000000298023224\n",
      "0.37933334708213806\n",
      "\n",
      "0.20000000298023224\n",
      "0.2266666740179062\n",
      "\n",
      "0.20000000298023224\n",
      "0.3956666588783264\n",
      "\n",
      "0.20000000298023224\n",
      "0.33899998664855957\n",
      "\n",
      "0.20000000298023224\n",
      "0.328000009059906\n",
      "\n",
      "0.20000000298023224\n",
      "0.36633333563804626\n",
      "\n",
      "0.20000000298023224\n",
      "0.30266666412353516\n",
      "\n",
      "0.20000000298023224\n",
      "0.34033334255218506\n",
      "\n",
      "0.20000000298023224\n",
      "0.3956666588783264\n",
      "\n",
      "0.20000000298023224\n",
      "0.3423333466053009\n",
      "\n",
      "0.20000000298023224\n",
      "0.2540000081062317\n",
      "\n",
      "0.20000000298023224\n",
      "0.23800000548362732\n",
      "\n",
      "0.20000000298023224\n",
      "0.2563333213329315\n",
      "\n",
      "0.20000000298023224\n",
      "0.3799999952316284\n",
      "\n",
      "0.20000000298023224\n",
      "0.31566667556762695\n",
      "\n",
      "0.20000000298023224\n",
      "0.3453333377838135\n",
      "\n",
      "0.20000000298023224\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-4193ec04bd50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtest_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     TASML.tasml_nn_classifier_learn(\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtest_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_tasks_for_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MEP2G17/Modules/COMP6248/CW/COMP6248-CW-MetaLearning-SP/tasml.py\u001b[0m in \u001b[0;36mtasml_nn_classifier_learn\u001b[0;34m(test_net, tasks, target_task, alpha_weights, convergence_diff, max_meta_epochs, inner_epochs, inner_lr, outer_lr, loss_function)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# Other loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mouter_loss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0malpha_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# Reload theta.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MEP2G17/Modules/COMP6248/CW/COMP6248-CW-MetaLearning-SP/TestNets.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Iterate each test task\n",
    "#Fetch the training tasks and weights for the test task\n",
    "for test_task_num, target_task in enumerate(test_tasks):\n",
    "    alpha_weights_for_target = alpha_weights[:,test_task_num]\n",
    "    training_tasks_for_target = train_tasks[test_task_num] #returns list\n",
    "    \n",
    "    for training_task_num, training_task in enumerate(training_tasks_for_target):\n",
    "        alpha_weight = alpha_weights_for_target[training_task_num]\n",
    "        support = training_task.task_classes[0].support_imgs # Get support set of class 0\n",
    "        query = training_task.task_classes[0].query_imgs\n",
    "        \n",
    "        if training_task_num < 5 and test_task_num==0: #Print weight and shape to get a feel of the data structures\n",
    "            print(\"alpha weight: \", alpha_weight, \"Support length: \", len(support), \"Query length: \", len(query))\n",
    "\n",
    "    test_net = get_test_net() \n",
    "    #\n",
    "    test_net.eval()\n",
    "    print((torch.sum(torch.argmax(test_net(target_task.query_train), dim=1) == target_task.query_targets) / target_task.query_targets.shape[0]).item())\n",
    "    test_net.train()\n",
    "    #\n",
    "    TASML.tasml_nn_classifier_learn(\n",
    "        test_net = test_net,\n",
    "        tasks=training_tasks_for_target,\n",
    "        target_task=target_task,\n",
    "        alpha_weights=alpha_weights_for_target,\n",
    "        # Tunables:\n",
    "        # convergence_diff=,\n",
    "        max_meta_epochs=10,\n",
    "        inner_epochs=1\n",
    "    )\n",
    "    test_net.eval()\n",
    "    print((torch.sum(torch.argmax(test_net(target_task.query_train), dim=1) == target_task.query_targets) / target_task.query_targets.shape[0]).item())\n",
    "    print()"
   ]
  },
  {
   "source": [
    "### MAML Runner:\n",
    "```python\n",
    "MAML.maml_nn_classifier_learn(\n",
    "    test_net: torch.nn.Module,\n",
    "    tasks: list[Task.Task],\n",
    "    convergence_diff: float = 0.0001,\n",
    "    max_meta_epochs = 10,\n",
    "    inner_epochs: int = 1,\n",
    "    inner_lr: float = 0.001,\n",
    "    outer_lr: float = 0.001,\n",
    "    loss_function = torch.nn.CrossEntropyLoss()):\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### TASML Runner:\n",
    "```python\n",
    "TASML.tasml_nn_classifier_learn(\n",
    "    test_net: torch.nn.Module,\n",
    "    tasks: list[Task.Task],\n",
    "    target_task: Task.Task,\n",
    "    alpha_weights: torch.Tensor,\n",
    "    convergence_diff: float = 0.0001,\n",
    "    max_meta_epochs = 10,\n",
    "    inner_epochs: int = 1,\n",
    "    inner_lr: float = 0.001,\n",
    "    outer_lr: float = 0.001,\n",
    "    loss_function = torch.nn.CrossEntropyLoss()):\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python394jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.9.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}